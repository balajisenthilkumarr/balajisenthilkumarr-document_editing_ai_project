import json
import os
import torch
import re
from transformers import T5ForConditionalGeneration, T5Tokenizer

# ‚úÖ Get project root directory dynamically
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# ‚úÖ Define absolute paths
MODEL_PATH = os.path.join(PROJECT_ROOT, "models", "t5_finetuned")
TEST_DATA_PATH = os.path.join(PROJECT_ROOT, "data", "test_dataset.json")

# ‚úÖ Ensure model files exist before loading
if not os.path.exists(os.path.join(MODEL_PATH, "model.safetensors")):
    print(f"‚ùå Model file not found in {MODEL_PATH}")
    exit()

print(f"‚úÖ Checking model files in {MODEL_PATH}...\n")
print(os.listdir(MODEL_PATH))  # Show all files in model directory

# ‚úÖ Load tokenizer from fine-tuned model
try:
    tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH, legacy=False)
    print("‚úÖ Tokenizer loaded successfully!")
except Exception as e:
    print(f"‚ùå Error loading tokenizer: {e}")
    exit()

# ‚úÖ Load fine-tuned model correctly
try:
    model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH)
    print("‚úÖ Model loaded successfully!")
except Exception as e:
    print(f"‚ùå Error loading model: {e}")
    exit()

# ‚úÖ Device setup (ensure inference is on GPU if available)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()

# ‚úÖ Function to normalize text for better matching
def normalize_text(text):
    text = text.lower().strip()
    text = re.sub(r"\s+", " ", text)  # Normalize multiple spaces
    text = text.replace("..", ".")  # Fix double periods
    return text

# ‚úÖ Function to check if model is just copying input
def is_copying_input(predicted_text, original_text):
    return normalize_text(predicted_text) == normalize_text(original_text)

# ‚úÖ Function to generate predictions with correct format
def generate_prediction(command, original_text):
    # ‚úÖ FIX: Use the same format as training
    formatted_input = f"Task: {command}\nText: {original_text}"

    print(f"\nüîç Debugging Tokenization for Input: {formatted_input}")
    inputs = tokenizer(formatted_input, return_tensors="pt", padding=True, truncation=True).to(device)

    print(f"üõ† Tokenized Input IDs: {inputs['input_ids']}")
    print(f"üõ† Tokenized Attention Mask: {inputs['attention_mask']}")

    with torch.no_grad():
        outputs = model.generate(
            **inputs, 
            max_length=512, 
            num_beams=5,  # Increased to improve results
            early_stopping=True
        )

    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

    if not predicted_text:
        print("‚ö†Ô∏è Warning: Model returned an empty output!")

    return predicted_text

# ‚úÖ Load test dataset
if os.path.exists(TEST_DATA_PATH):
    with open(TEST_DATA_PATH, "r", encoding="utf-8") as file:
        test_data = json.load(file)
    print(f"‚úÖ Loaded {len(test_data)} test cases from {TEST_DATA_PATH}.")
else:
    print("‚ö†Ô∏è Test dataset not found!")
    exit()

# ‚úÖ Running a manual test before evaluation
print("\nüîé Running a manual test...")

sample_command = "Replace 'CEO' with 'Managing Director'"
sample_text = "The CEO will address the employees."
predicted_output = generate_prediction(sample_command, sample_text)

print("\nüîé Model Output:", repr(predicted_output))

# ‚úÖ Evaluate the model
correct_predictions = 0
total_samples = len(test_data)
incorrect_cases = []
copying_cases = []

print("\n‚úÖ Running model evaluation...\n")

for i, entry in enumerate(test_data):
    try:
        command = entry["command"]
        original_text = entry["original_text"]
        expected_output = entry["modified_text"]

        predicted_output = generate_prediction(command, original_text)

        print(f"\nüîπ Test {i+1}/{total_samples}")
        print(f"üìù Command: {command}")
        print(f"üìå Original: {original_text}")
        print(f"‚úÖ Expected: {expected_output}")
        print(f"üîé Predicted: {predicted_output}")

        # Apply normalization before comparison
        if normalize_text(predicted_output) == normalize_text(expected_output):
            correct_predictions += 1
        else:
            print("‚ùå Mismatch detected!")
            incorrect_cases.append({
                "command": command,
                "original": original_text,
                "expected": expected_output,
                "predicted": predicted_output
            })

        # Check if the model is copying input text
        if is_copying_input(predicted_output, original_text):
            print("üö® Model is copying input instead of modifying!")
            copying_cases.append({
                "command": command,
                "original": original_text,
                "expected": expected_output,
                "predicted": predicted_output
            })

    except ValueError:
        print(f"‚ö†Ô∏è Skipping test {i+1}: Invalid format in test dataset")

# ‚úÖ Print evaluation results
accuracy = (correct_predictions / total_samples) * 100
print(f"\n‚úÖ Model Evaluation Complete! Accuracy: {accuracy:.2f}%")

# ‚úÖ Save incorrect cases for debugging
if incorrect_cases:
    debug_path = os.path.join(PROJECT_ROOT, "data", "incorrect_predictions.json")
    with open(debug_path, "w", encoding="utf-8") as debug_file:
        json.dump(incorrect_cases, debug_file, indent=4)
    print(f"‚ö†Ô∏è {len(incorrect_cases)} incorrect predictions saved to {debug_path} for review.")
else:
    print("üéâ All predictions matched! No incorrect cases.")

# ‚úÖ Save cases where the model is copying input
if copying_cases:
    copy_debug_path = os.path.join(PROJECT_ROOT, "data", "copying_predictions.json")
    with open(copy_debug_path, "w", encoding="utf-8") as copy_debug_file:
        json.dump(copying_cases, copy_debug_file, indent=4)
    print(f"üö® {len(copying_cases)} cases where the model is copying input saved to {copy_debug_path}.")

print("\n‚úÖ Testing complete!")
